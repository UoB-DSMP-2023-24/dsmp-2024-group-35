{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#Loading the dataset into a dataframe\n",
    "df = pd.read_csv('C:/Users/sheth/Desktop/DSMP/simulated_transaction_2024.csv')\n",
    "\n",
    "#Having a look at the data\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Frequency of transactions for each unique value in 'Third party name' column\n",
    "transaction_counts = df['Third Party Name'].value_counts()\n",
    "pd.set_option('display.max_rows', None)\n",
    "print(transaction_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Engineering as done in Dataset 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining a function to create a new feature to catagorize the transactions into filtered categories\n",
    "def categorize(name):\n",
    "    if name in ['Sports Direct', 'Topshop', 'Fat Face', 'Matalan', 'AMAZON', 'Blackwell\\'s','Reebok', 'JD Sports', 'North Face', 'Boots', 'Lloyds Pharmacy']:\n",
    "        return 'Retail Stores'\n",
    "    elif name in ['Netflix', 'Disney', 'Blizzard', 'Mojang Studios', 'Amazon', 'Xbox', 'Gamestation']:\n",
    "        return 'Entertainment & Media'\n",
    "    elif name in ['Sainsbury', 'Tesco', 'Coop Local', 'Sainsbury Local']:\n",
    "        return 'Grocery'\n",
    "    elif name in ['JustEat', 'Deliveroo', 'Starbucks', 'Five Senses Art', 'Coffee #1', 'Costa Coffee', 'Jollyes']:\n",
    "        return 'Food & Dining'\n",
    "    elif name in ['Halifax', 'LBG', 'Premier Finance', 'CPA']:\n",
    "        return 'Financial Services'\n",
    "    elif name in ['PureGym', 'Grand Union BJJ', 'Selfridges', 'Lloyds Pharmacy', 'Vision Express', 'Pets Corner']:\n",
    "        return 'Health & Wellness'\n",
    "    elif name in ['Blackwell\\'s', 'Brilliant Brushes','Craftastic', 'A Yarn Story', 'Cass Art', 'Foyles']:\n",
    "        return 'Education & Books'\n",
    "    elif name in ['The Works', 'Loosely Fitted', 'Wool', 'Hobby Lobby', 'Hobbycraft', 'Happy Days Home', 'Lavender Fields']:\n",
    "        return 'Home & Lifestyle'\n",
    "    else:\n",
    "        return 'Other Services'\n",
    "\n",
    "#Creating a new feature 'Category' based on the function\n",
    "df['Category'] = df['Third Party Name'].apply(categorize)\n",
    "\n",
    "#DataFrame with the new 'Category' column\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To check which Third Parties havent been categorised \n",
    "#Transactions where 'Category' is 'Other Services' and display all values\n",
    "other_services_df = df[df['Category'] == 'Other Services']\n",
    "print(other_services_df['Third Party Name'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert 'Amount' column to numeric\n",
    "df['Amount'] = pd.to_numeric(df['Amount'], errors='coerce')\n",
    "\n",
    "#Average, minimum, and maximum spends\n",
    "category_stats = df.groupby('Category')['Amount'].agg(['mean', 'min', 'max'])\n",
    "\n",
    "# Display the statistics\n",
    "print(category_stats)\n",
    "\n",
    "#Function for the transaction with minimum spend\n",
    "def get_min_transaction_details(group):\n",
    "    min_transaction = group.loc[group['Amount'] == group['Amount'].min()]\n",
    "    return min_transaction[['Third Party Name', 'Amount']]\n",
    "\n",
    "#Function for the transaction with maximum spend\n",
    "def get_max_transaction_details(group):\n",
    "    max_transaction = group.loc[group['Amount'] == group['Amount'].max()]\n",
    "    return max_transaction[['Third Party Name', 'Amount']]\n",
    "\n",
    "#Transactions with min and max spends for each category\n",
    "min_transaction_details = df.groupby('Category').apply(get_min_transaction_details)\n",
    "max_transaction_details = df.groupby('Category').apply(get_max_transaction_details)\n",
    "\n",
    "#Statistics and transaction details\n",
    "for category, stats in category_stats.iterrows():\n",
    "    print(f\"Category: {category}\")\n",
    "    print(f\"Average Spend: {stats['mean']:.2f}\")\n",
    "    print(f\"Minimum Spend: {stats['min']:.2f}\")\n",
    "    print(\"Details of Minimum Spend Transaction:\")\n",
    "    print(min_transaction_details.loc[category])\n",
    "    print(f\"Maximum Spend: {stats['max']:.2f}\")\n",
    "    print(\"Details of Maximum Spend Transaction:\")\n",
    "    print(max_transaction_details.loc[category])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Average, minimum, and maximum spends\n",
    "category_stats = df.groupby('Category')['Amount'].agg(['mean', 'min', 'max']).round(2)\n",
    "\n",
    "#Plotting mean of transactions by Category\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(category_stats.index, category_stats['mean'], color='skyblue')\n",
    "plt.title('Mean Transaction Amounts by Category')\n",
    "plt.xlabel('Category')\n",
    "plt.ylabel('Mean Amount')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#Plotting min of transactions by Category\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(category_stats.index, category_stats['min'], color='lightgreen')\n",
    "plt.title('Minimum Transaction Amounts by Category')\n",
    "plt.xlabel('Category')\n",
    "plt.ylabel('Minimum Amount')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#Plotting max of transactions by Category\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(category_stats.index, category_stats['max'], color='salmon')\n",
    "plt.title('Maximum Transaction Amounts by Category')\n",
    "plt.xlabel('Category')\n",
    "plt.ylabel('Maximum Amount')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variance for each category\n",
    "category_variance = df.groupby('Category')['Amount'].var()\n",
    "\n",
    "#Rounding the variances to three decimal places\n",
    "category_variance = category_variance.round(3)\n",
    "\n",
    "#Variance for each category\n",
    "print(category_variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting variances by Categoeies\n",
    "category_variance.plot(kind='bar', color='skyblue', figsize=(10, 6))\n",
    "plt.title('Variance of Transaction Amounts by Category')\n",
    "plt.xlabel('Category')\n",
    "plt.ylabel('Variance')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proceeding with RFM Analysis.\n",
    " This analysis involves allocationn of individual scores based on:\n",
    " * (R) recency\n",
    " * (F) frequency\n",
    " * (M) Monetary  \n",
    " \n",
    " We have used the metric, 'Quantile' to calculate the score of each of the 3 aspects, where lower than 25% is given a score of 4, lower than 50%, a 3 and so on. After calculating all the individual scores, we sum them to get a RFM score, the customers with the best RFM scores are our target customers for any new services, projects, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting 'Date' column to datetime format\n",
    "df['Date'] = pd.to_datetime(df['Date'], format='%d/%m/%Y')\n",
    "\n",
    "#Calculating recency, frequency, and monetary value for each transaction\n",
    "snapshot_date = df['Date'].max() + pd.DateOffset(1)  \n",
    "rfm_df = df.groupby('Account No').agg({\n",
    "    'Date': lambda x: (snapshot_date - x.max()).days,  \n",
    "    'Account No': 'count',                             \n",
    "    'Amount': 'sum'                                    \n",
    "}).rename(columns={'Date': 'Recency', 'Account No': 'Frequency', 'Amount': 'Monetary'})\n",
    "\n",
    "#Calculating RFM scores for each transaction\n",
    "rfm_scores = df.groupby('Account No').agg({\n",
    "    'Date': lambda x: (snapshot_date - x.max()).days,  \n",
    "    'Account No': 'count',                             \n",
    "    'Amount': 'sum'                                    \n",
    "}).rename(columns={'Date': 'Recency', 'Account No': 'Frequency', 'Amount': 'Monetary'})\n",
    "\n",
    "#Defining quantiles for RFM scores\n",
    "quantiles = rfm_scores.quantile(q=[0.25, 0.5, 0.75])\n",
    "\n",
    "#Assigning scores based on quantiles\n",
    "def assign_rfm_score(x, c, quantiles):\n",
    "    if x <= quantiles[c][0.25]:\n",
    "        return 4\n",
    "    elif x <= quantiles[c][0.50]:\n",
    "        return 3\n",
    "    elif x <= quantiles[c][0.75]: \n",
    "        return 2\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "rfm_scores['R_Score'] = rfm_scores['Recency'].apply(assign_rfm_score, args=('Recency', quantiles))\n",
    "rfm_scores['F_Score'] = rfm_scores['Frequency'].apply(assign_rfm_score, args=('Frequency', quantiles))\n",
    "rfm_scores['M_Score'] = rfm_scores['Monetary'].apply(assign_rfm_score, args=('Monetary', quantiles))\n",
    "\n",
    "#RFM scores for each transaction\n",
    "print(rfm_scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following are the Recency, Frequency and Monetary Scores for the unique accounts of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RFM scores\n",
    "print(rfm_scores[['R_Score', 'F_Score', 'M_Score']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summing up RFM Scores to get best customers from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Calculate Total RFM score\n",
    "rfm_scores['RFM_Score'] = rfm_scores['R_Score'] + rfm_scores['F_Score'] + rfm_scores['M_Score']\n",
    "\n",
    "#Sorting customers by RFM score in descending order to get the best customers\n",
    "best_customers = rfm_scores.sort_values(by='RFM_Score', ascending=False)\n",
    "\n",
    "#Best customers\n",
    "print(best_customers.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Top 1000 customers\n",
    "top_1000_customers = best_customers.head(1000)\n",
    "print(top_1000_customers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the distribution of Actual number of accounts over the transactions, to identify the High-Value, Loyal and Lost customers.\n",
    "* High-Value Customers: Account holders with high Frequency and Monetary Transactions, low Recency\n",
    "* Loyal Customers: Account holders with high Recency and Frequency Transactions, low Monetary\n",
    "* Lost Customers: Account holders with low Recency and Frequency Transactions, high Monetary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_accounts_count = len(df['Account No'].value_counts())\n",
    "print(\"Unique number of accounts:\", unique_accounts_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Threshold values\n",
    "recency_threshold_high = 2  \n",
    "frequency_threshold_high = 2  \n",
    "monetary_threshold_high = 2  \n",
    "\n",
    "frequency_threshold_loyal = 2  \n",
    "monetary_threshold_loyal = 2  \n",
    "\n",
    "recency_threshold_lost = 3  \n",
    "frequency_threshold_lost = 3  \n",
    "monetary_threshold_lost = 3  \n",
    "\n",
    "#Reseting the 'RFM_Segment' column\n",
    "rfm_scores['RFM_Segment'] = ''\n",
    "\n",
    "# Assign segment labels based on the adjusted thresholds to the entire dataset\n",
    "rfm_scores.loc[(rfm_scores['R_Score'] <= recency_threshold_high) & \n",
    "               (rfm_scores['F_Score'] >= frequency_threshold_high) & \n",
    "               (rfm_scores['M_Score'] >= monetary_threshold_high), 'RFM_Segment'] = 'High-Value Customer'\n",
    "\n",
    "rfm_scores.loc[(rfm_scores['F_Score'] >= frequency_threshold_loyal) & \n",
    "               (rfm_scores['R_Score'] >= recency_threshold_high) & \n",
    "               (rfm_scores['M_Score'] <= monetary_threshold_lost) &\n",
    "               (rfm_scores['RFM_Segment'] == ''), 'RFM_Segment'] = 'Loyal Customer'\n",
    "\n",
    "rfm_scores.loc[(rfm_scores['R_Score'] <= recency_threshold_lost) & \n",
    "               (rfm_scores['F_Score'] <= frequency_threshold_lost) & \n",
    "               (rfm_scores['M_Score'] >= monetary_threshold_lost) &\n",
    "               (rfm_scores['RFM_Segment'] == ''), 'RFM_Segment'] = 'Lost Customer'\n",
    "\n",
    "#Number of customers in each segment\n",
    "segment_counts = rfm_scores['RFM_Segment'].value_counts()\n",
    "print(segment_counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering: K-means clustering and DBSCAN can be applied to segment customers based on their transactional behavior.By clustering customers based on transaction attributes such as Balance, Amount, and Category, we can identify groups of customers with similar spending patterns or preferences. Eg: High spenders, frequent shoppers, or users of specific services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values = df.isnull().sum()\n",
    "print(missing_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "#Dropping rows with missing values in Date, Timestamp, and Account No columns\n",
    "df.dropna(subset=[\"Date\", \"Timestamp\", \"Account No\"], inplace=True)\n",
    "\n",
    "#Imputing missing values in numeric columns (Amount, Balance) with median\n",
    "numeric_cols = [\"Amount\", \"Balance\"]\n",
    "numeric_imputer = SimpleImputer(strategy=\"median\")\n",
    "df[numeric_cols] = numeric_imputer.fit_transform(df[numeric_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data types of all columns\n",
    "print(df.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting \"Timestamp\" column to datetime64[ns]\n",
    "df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#Extracting features from Date and Timestamp columns\n",
    "df['Year'] = df['Date'].dt.year\n",
    "df['Month'] = df['Date'].dt.month\n",
    "df['Day'] = df['Date'].dt.day\n",
    "df['Hour'] = df['Timestamp'].dt.hour\n",
    "df['Minute'] = df['Timestamp'].dt.minute\n",
    "\n",
    "#Dropping the original Date and Timestamp columns\n",
    "df.drop(columns=['Date', 'Timestamp'], inplace=True)\n",
    "\n",
    "#Applying one-hot encoding to the Category column\n",
    "df_encoded = pd.get_dummies(df, columns=['Category'], drop_first=True)\n",
    "\n",
    "#Dataframe with encoded columns\n",
    "print(df_encoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping rows with NaN values\n",
    "df_cleaned = df_without_name.dropna()\n",
    "\n",
    "#Standardization\n",
    "scaler = StandardScaler()\n",
    "scaled_df = scaler.fit_transform(df_cleaned)\n",
    "\n",
    "#Initializing KMeans model\n",
    "kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "\n",
    "#Fitting KMeans model to the data\n",
    "kmeans.fit(scaled_df)\n",
    "\n",
    "#Cluster labels\n",
    "cluster_labels = kmeans.labels_\n",
    "\n",
    "#Concatinating cluster labels to the dataframe\n",
    "df_cleaned['Cluster'] = cluster_labels\n",
    "\n",
    "#Dataframe with cluster labels\n",
    "print(df_cleaned)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#Visualizing Balance and Amount features with clusters\n",
    "feature1 = 'Balance'\n",
    "feature2 = 'Amount'\n",
    "\n",
    "#Plotting the data points\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x=feature1, y=feature2, hue='Cluster', data=df_cleaned, palette='viridis', legend='full')\n",
    "plt.title('Clustering Output')\n",
    "plt.xlabel(feature1)\n",
    "plt.ylabel(feature2)\n",
    "\n",
    "#Adding cluster centers\n",
    "if hasattr(kmeans, 'cluster_centers_'):\n",
    "    cluster_centers = scaler.inverse_transform(kmeans.cluster_centers_)  \n",
    "    plt.scatter(cluster_centers[:, df_cleaned.columns.get_loc(feature1)], \n",
    "                cluster_centers[:, df_cleaned.columns.get_loc(feature2)], \n",
    "                marker='x', s=100, c='black', label='Cluster Centers')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Initializing DBSCAN model\n",
    "dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
    "\n",
    "#Fitting DBSCAN model to the data\n",
    "dbscan.fit(scaled_df)\n",
    "\n",
    "#Cluster labels\n",
    "cluster_labels = dbscan.labels_\n",
    "\n",
    "#Cluster labels to the dataframe\n",
    "df_cleaned['Cluster'] = cluster_labels\n",
    "\n",
    "#Dataframe with cluster labels\n",
    "print(df_cleaned)\n",
    "\n",
    "#Visualizing the clusters\n",
    "plt.scatter(df_cleaned['Balance'], df_cleaned['Amount'], c=cluster_labels, cmap='viridis')\n",
    "plt.xlabel('Balance')\n",
    "plt.ylabel('Amount')\n",
    "plt.title('DBSCAN Clustering')\n",
    "plt.colorbar(label='Cluster')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Create subplots\n",
    "fig, axs = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "#Plot for Balance and Amount\n",
    "axs[0].scatter(df_cleaned['Balance'], df_cleaned['Amount'], c=cluster_labels, cmap='viridis')\n",
    "axs[0].set_xlabel('Balance')\n",
    "axs[0].set_ylabel('Amount')\n",
    "axs[0].set_title('Balance vs Amount')\n",
    "\n",
    "#Plot for Account No and Amount\n",
    "axs[1].scatter(df_cleaned['Account No'], df_cleaned['Amount'], c=cluster_labels, cmap='viridis')\n",
    "axs[1].set_xlabel('Account No')\n",
    "axs[1].set_ylabel('Amount')\n",
    "axs[1].set_title('Account No vs Amount')\n",
    "\n",
    "#Plot for Account No and Balance\n",
    "axs[2].scatter(df_cleaned['Account No'], df_cleaned['Balance'], c=cluster_labels, cmap='viridis')\n",
    "axs[2].set_xlabel('Account No')\n",
    "axs[2].set_ylabel('Balance')\n",
    "axs[2].set_title('Account No vs Balance')\n",
    "\n",
    "#Visualizing the Plots\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df_cleaned['Balance'], df_cleaned['Amount'], c=cluster_labels, cmap='viridis')\n",
    "plt.xlabel('Balance')\n",
    "plt.ylabel('Amount')\n",
    "plt.title('Balance vs Amount (Clustered)')\n",
    "plt.colorbar(label='Cluster')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df_cleaned['Amount'], bins=30, color='skyblue', alpha=0.7)\n",
    "plt.xlabel('Amount')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Transaction Amount Distribution (Clustered)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing the optimum number of clusters using the Elbow method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "#To define the optimum number of clusters \n",
    "k_values = range(1, 11)\n",
    "wcss = []\n",
    "\n",
    "#Calculating WCSS for each k\n",
    "for k in k_values:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(scaled_df)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "\n",
    "#Plotting the WCSS against the number of clusters\n",
    "plt.plot(k_values, wcss, marker='*')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Within-Cluster Sum of Squares (WCSS)')\n",
    "plt.title('Elbow Method')\n",
    "plt.xticks(k_values)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Moving with 3 clusters\n",
    "k = 3\n",
    "\n",
    "#Reinitializing and refitting KMeans model with 3 clusters\n",
    "kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "kmeans.fit(scaled_df)\n",
    "\n",
    "#Updating cluster labels\n",
    "cluster_labels = kmeans.labels_\n",
    "\n",
    "#Updating dataframe with cluster labels\n",
    "df_cleaned['Cluster'] = cluster_labels\n",
    "\n",
    "#Dataframe with updated cluster labels\n",
    "print(df_cleaned)\n",
    "\n",
    "#Visualizing Balance and Amount features with 3 clusters\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x=feature1, y=feature2, hue='Cluster', data=df_cleaned, palette='viridis', legend='full')\n",
    "plt.title('Clustering Output (3 Clusters)')\n",
    "plt.xlabel(feature1)\n",
    "plt.ylabel(feature2)\n",
    "\n",
    "#Cluster centers\n",
    "if hasattr(kmeans, 'cluster_centers_'):\n",
    "    cluster_centers = scaler.inverse_transform(kmeans.cluster_centers_)  \n",
    "    plt.scatter(cluster_centers[:, df_cleaned.columns.get_loc(feature1)], \n",
    "                cluster_centers[:, df_cleaned.columns.get_loc(feature2)], \n",
    "                marker='x', s=100, c='black', label='Cluster Centers')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Moving with 4 clusters\n",
    "k = 4\n",
    "\n",
    "#Reinitializing and refitting KMeans model with 4 clusters\n",
    "kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "kmeans.fit(scaled_df)\n",
    "\n",
    "#Updating cluster labels\n",
    "cluster_labels = kmeans.labels_\n",
    "\n",
    "#Updating dataframe with cluster labels\n",
    "df_cleaned['Cluster'] = cluster_labels\n",
    "\n",
    "#Dataframe with updated cluster labels\n",
    "print(df_cleaned)\n",
    "\n",
    "#Visualizing Balance and Amount features with 4 clusters\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x=feature1, y=feature2, hue='Cluster', data=df_cleaned, palette='viridis', legend='full')\n",
    "plt.title('Clustering Output (4 Clusters)')\n",
    "plt.xlabel(feature1)\n",
    "plt.ylabel(feature2)\n",
    "\n",
    "#Cluster centers\n",
    "if hasattr(kmeans, 'cluster_centers_'):\n",
    "    cluster_centers = scaler.inverse_transform(kmeans.cluster_centers_)  \n",
    "    plt.scatter(cluster_centers[:, df_cleaned.columns.get_loc(feature1)], \n",
    "                cluster_centers[:, df_cleaned.columns.get_loc(feature2)], \n",
    "                marker='x', s=100, c='black', label='Cluster Centers')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Moving with 5 clusters\n",
    "k = 5\n",
    "\n",
    "#Reinitialize and refit KMeans model with 5 clusters\n",
    "kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "kmeans.fit(scaled_df)\n",
    "\n",
    "#Updating cluster labels\n",
    "cluster_labels = kmeans.labels_\n",
    "\n",
    "#Updating dataframe with cluster labels\n",
    "df_cleaned['Cluster'] = cluster_labels\n",
    "\n",
    "#Dataframe with updated cluster labels\n",
    "print(df_cleaned)\n",
    "\n",
    "#Visualizing Balance and Amount features with 5 clusters\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x=feature1, y=feature2, hue='Cluster', data=df_cleaned, palette='viridis', legend='full')\n",
    "plt.title('Clustering Output (5 Clusters)')\n",
    "plt.xlabel(feature1)\n",
    "plt.ylabel(feature2)\n",
    "\n",
    "#Cluster centers\n",
    "if hasattr(kmeans, 'cluster_centers_'):\n",
    "    cluster_centers = scaler.inverse_transform(kmeans.cluster_centers_)  \n",
    "    plt.scatter(cluster_centers[:, df_cleaned.columns.get_loc(feature1)], \n",
    "                cluster_centers[:, df_cleaned.columns.get_loc(feature2)], \n",
    "                marker='x', s=100, c='black', label='Cluster Centers')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time Series Forecasting (ARIMA, LSTM): Time series forecasting techniques such as ARIMA (AutoRegressive Integrated Moving Average) or LSTM (Long Short-Term Memory) neural networks can be used to predict future transaction volumes or identify seasonal trends in customer behavior. They can guide resource allocation, marketing campaign planning, and inventory management."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Customer Churn Prediction: Logistic Regression, Random Forest, or Gradient Boosting Machines (GBM).\n",
    "These models can forecast the likelihood of customers discontinuing their relationship with the bank based on their transaction history and behavior. By identifying customers at risk of churn, the bank can implement retention strategies to retain valuable customers and reduce churn rates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Learning for Anomaly Detection: Deep learning models, such as autoencoders or variational autoencoders (VAEs), could be trained on transactional data to detect anomalies or unusual patterns that may indicate fraudulent activities or outliers in customer behavior."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
