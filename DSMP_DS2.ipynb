{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"C:/Users/sheth/Desktop/DSMP/simulated_transaction_2024.csv\")\n",
    "(df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Frequency of transactions for each unique value in 'Third party name' column\n",
    "transaction_counts = df['Third Party Name'].value_counts()\n",
    "pd.set_option('display.max_rows', None)\n",
    "print(transaction_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Engineering as done in Dataset 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining a function to create a new feature to catagorize the transactions into filtered categories\n",
    "def categorize(name):\n",
    "    if name in ['Sports Direct','Mamas & Papas','Head','Gap Kids','Millets','HMV', 'Collector Cave', 'Etsy','Topshop', 'Fat Face', 'Matalan','Specsavers','Barbiee Boutique','Revella', 'AMAZON', 'Blackwell\\'s','Reebok', 'JD Sports', 'North Face', 'Boots', 'Lloyds Pharmacy']:\n",
    "        return 'Retail Stores'\n",
    "    elif name in ['Netflix', 'Disney', 'Blizzard', 'Mojang Studios', 'Amazon', 'Xbox', 'Gamestation','A Cut Above',\n",
    "                  'The Crown']:\n",
    "        return 'Entertainment & Media'\n",
    "    elif name in ['Sainsbury', 'Tesco', 'Coop Local', 'Sainsbury Local']:\n",
    "        return 'Grocery'\n",
    "    elif name in ['JustEat', 'Deliveroo', 'Starbucks', 'Five Senses Art', 'Coffee #1', 'Costa Coffee', 'Jollyes','Rose & Crown',\n",
    "                  'Kings Arms','Frankie & Bennies']:\n",
    "        return 'Food & Dining'\n",
    "    elif name in ['Halifax', 'LBG', 'Premier Finance', 'CPA']:\n",
    "        return 'Financial Services'\n",
    "    elif name in ['PureGym', 'Grand Union BJJ', 'Selfridges','Mothercare', 'Lloyds Pharmacy',  'RugbyFields','Sunny Care Nursery', 'Remedy plus care',\n",
    "                  'Vision Express', 'Pets Corner','University College Hospital']:\n",
    "        return 'Health & Wellness'\n",
    "    elif name in ['Blackwell\\'s', 'Brilliant Brushes','Craftastic', 'A Yarn Story', 'Cass Art', 'Foyles','Lavender Primary',\n",
    "                  'Green Park Academy']:\n",
    "        return 'Education & Books'\n",
    "    elif name in ['The Works', 'Loosely Fitted', 'Wool', 'Hobby Lobby', 'Hobbycraft', 'Happy Days Home', 'Lavender Fields']:\n",
    "        return 'Home & Lifestyle'\n",
    "    else:\n",
    "        return 'Other Services'\n",
    "\n",
    "#Creating a new feature 'Category' based on the function\n",
    "df['Category'] = df['Third Party Name'].apply(categorize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting 'Amount' column to numeric, so as to perform mathematical functions     \n",
    "df['Amount'] = pd.to_numeric(df['Amount'], errors='coerce')\n",
    "\n",
    "#Average, minimum, and maximum spends\n",
    "category_stats = df.groupby('Category')['Amount'].agg(['mean', 'min', 'max'])\n",
    "print(category_stats)\n",
    "\n",
    "#Function for the transaction with minimum spend\n",
    "def get_min_transaction_details(group):\n",
    "    min_transaction = group.loc[group['Amount'] == group['Amount'].min()]\n",
    "    return min_transaction[['Third Party Name', 'Amount']]\n",
    "\n",
    "#Function for the transaction with maximum spend\n",
    "def get_max_transaction_details(group):\n",
    "    max_transaction = group.loc[group['Amount'] == group['Amount'].max()]\n",
    "    return max_transaction[['Third Party Name', 'Amount']]\n",
    "\n",
    "#Transactions with min and max spends for each category\n",
    "min_transaction_details = df.groupby('Category').apply(get_min_transaction_details)\n",
    "max_transaction_details = df.groupby('Category').apply(get_max_transaction_details)\n",
    "\n",
    "#Statistics and transaction details\n",
    "for category, stats in category_stats.iterrows():\n",
    "    print(f\"Category: {category}\")\n",
    "    print(f\"Average Spend: {stats['mean']:.2f}\")\n",
    "    print(f\"Minimum Spend: {stats['min']:.2f}\")\n",
    "    print(\"Details of Minimum Spend Transaction:\")\n",
    "    print(min_transaction_details.loc[category])\n",
    "    print(f\"Maximum Spend: {stats['max']:.2f}\")\n",
    "    print(\"Details of Maximum Spend Transaction:\")\n",
    "    print(max_transaction_details.loc[category])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Average, minimum, and maximum spends\n",
    "category_stats = df.groupby('Category')['Amount'].agg(['mean', 'min', 'max']).round(2)\n",
    "\n",
    "#Plotting mean of transactions by Category\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(category_stats.index, category_stats['mean'], color='skyblue')\n",
    "plt.title('Mean Transaction Amounts by Category')\n",
    "plt.xlabel('Category')\n",
    "plt.ylabel('Mean Amount')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#Plotting min of transactions by Category\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(category_stats.index, category_stats['min'], color='lightgreen')\n",
    "plt.title('Minimum Transaction Amounts by Category')\n",
    "plt.xlabel('Category')\n",
    "plt.ylabel('Minimum Amount')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#Plotting max of transactions by Category\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(category_stats.index, category_stats['max'], color='salmon')\n",
    "plt.title('Maximum Transaction Amounts by Category')\n",
    "plt.xlabel('Category')\n",
    "plt.ylabel('Maximum Amount')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variance for each category\n",
    "category_variance = df.groupby('Category')['Amount'].var()\n",
    "\n",
    "#Rounding the variances to three decimal places\n",
    "category_variance = category_variance.round(3)\n",
    "print(category_variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting variances by Categoeies\n",
    "category_variance.plot(kind='bar', color='skyblue', figsize=(10, 6))\n",
    "plt.title('Variance of Transaction Amounts by Category')\n",
    "plt.xlabel('Category')\n",
    "plt.ylabel('Variance')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  RFM Analysis\n",
    " This analysis involves allocationn of individual scores based on:\n",
    " * (R) recency\n",
    " * (F) frequency\n",
    " * (M) Monetary  \n",
    " \n",
    " We have used the metric, 'Quantile' to calculate the score of each of the 3 aspects, where lower than 25% is given a score of 4, lower than 50%, a 3 and so on. After calculating all the individual scores, we sum them to get a RFM score, the customers with the best RFM scores are our target customers for any new services, projects, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Date'] = pd.to_datetime(df['Date'], format='%d/%m/%Y')\n",
    "\n",
    "#Calculating the most recent date plus one day for snapshot_date\n",
    "snapshot_date = df['Date'].max() + pd.DateOffset(days=1)\n",
    "\n",
    "rfm_scores = df.groupby('Account No').agg({\n",
    "    'Date': lambda x: (snapshot_date - x.max()).days,  # Recency: Days since last transaction\n",
    "    'Account No': 'count',  # Frequency: Total number of transactions\n",
    "    'Amount': 'sum'  # Monetary: Sum of transaction values\n",
    "}).rename(columns={'Date': 'Recency', 'Account No': 'Frequency', 'Amount': 'Monetary'})\n",
    "\n",
    "quantiles = rfm_scores.quantile(q=[0.25, 0.5, 0.75])\n",
    "\n",
    "\n",
    "def assign_rfm_score(x, c, quantiles):\n",
    "    if x <= quantiles.loc[0.25, c]:\n",
    "        return 4 if c == 'Recency' else 1  # Lower recency is better, hence 4 is best\n",
    "    elif x <= quantiles.loc[0.5, c]:\n",
    "        return 3 if c == 'Recency' else 2\n",
    "    elif x <= quantiles.loc[0.75, c]:\n",
    "        return 2 if c == 'Recency' else 3\n",
    "    else:\n",
    "        return 1 if c == 'Recency' else 4  # Higher recency is worse, hence 1 is worst\n",
    "\n",
    "rfm_scores['R_Score'] = rfm_scores['Recency'].apply(assign_rfm_score, args=('Recency', quantiles))\n",
    "rfm_scores['F_Score'] = rfm_scores['Frequency'].apply(assign_rfm_score, args=('Frequency', quantiles))\n",
    "rfm_scores['M_Score'] = rfm_scores['Monetary'].apply(assign_rfm_score, args=('Monetary', quantiles))\n",
    "\n",
    "#Summing the scores to create the RFM Score\n",
    "rfm_scores['RFM_Score'] = rfm_scores['R_Score'] + rfm_scores['F_Score'] + rfm_scores['M_Score']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summing up R,F, and M Scores to get best customers from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate Total RFM score\n",
    "rfm_scores['RFM_Score'] = rfm_scores['R_Score'] + rfm_scores['F_Score'] + rfm_scores['M_Score']\n",
    "\n",
    "#Sorting customers by RFM score in descending order to get the best customers\n",
    "best_customers = rfm_scores.sort_values(by='RFM_Score', ascending=False)\n",
    "best_customers.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 500 customers \n",
    "top_500_customers = best_customers.head(500)\n",
    "top_500_customers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the distribution of Actual number of accounts over the transactions, to identify:\n",
    "* High-Value Customers: Accounts with the highest scores in recency, frequency, and monetary values (4 on all scores)\n",
    "* Loyal Customers: Accounts with high frequency and monetary scores (3-4), regardless of their recency score\n",
    "* Emerging Customers: Accounts with the highest recency score (4) but lower frequency and monetary scores (1-3)\n",
    "* Risk Customers: Accounts with low recency scores (1-2) but high frequency and monetary scores (3-4)\n",
    "* Lost Customers:  Accounts with low scores across recency, frequency, and monetary (1-2)\n",
    "* Need Attention Customers: Accounts with medium scores in all categories (2-3)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfm_scores.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_accounts_count = len(df['Account No'].value_counts())\n",
    "print(\"Unique number of accounts:\", unique_accounts_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining a function to classify customers as per the set thresholds\n",
    "def classify_customer(row):\n",
    "    r, f, m = row['R_Score'], row['F_Score'], row['M_Score']\n",
    "    if r == 4 and f == 4 and m == 4:\n",
    "        return 'High Value'\n",
    "    elif (f == 3 or f == 4) and (m == 3 or m == 4):\n",
    "        return 'Loyal Customers'\n",
    "    elif r == 4 and (f in [1, 2, 3]) and (m in [1, 2, 3]):\n",
    "        return 'Emerging Customers'\n",
    "    elif (r == 1 or r == 2) and (f in [3, 4]) and (m in [3, 4]):\n",
    "        return 'At Risk'\n",
    "    elif (r in [1, 2]) and (f in [1, 2]) and (m in [1, 2]):\n",
    "        return 'Lost Customers'\n",
    "    else:\n",
    "        return 'Need Attention'\n",
    "\n",
    "#Applying the classification function\n",
    "rfm_scores['Customer Segment'] = rfm_scores.apply(classify_customer, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfm_scores['Customer Segment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "customer_segments = ['High Value','Loyal Customers', 'Emerging Customers','Lost Customers', 'Need Attention']\n",
    "counts = [146,240,82,290,218]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(customer_segments, counts, color=['#B3CDE3', '#FED9A6', '#CCEBC5', '#FDDAEC', '#F2F2F2'])\n",
    "plt.title('Customer Segment Distribution')\n",
    "plt.xlabel('Customer Segment')\n",
    "plt.ylabel('Number of Customers')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()  \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfm_scores['Customer Segment'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering: K-means clustering and DBSCAN can be applied to segment customers based on their transactional behavior.By clustering customers based on transaction attributes such as Balance, Amount, and Category, we can identify groups of customers with similar spending patterns or preferences. Eg: High spenders, frequent shoppers, or users of specific services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values = df.isnull().sum()\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "#Group by 'Account No' and replace NaNs with the median for each group, so that all NaNs are filled with the median of their own remaining transactions\n",
    "grouped = df.groupby('Account No')\n",
    "df['Balance'] = grouped['Balance'].transform(lambda x: x.fillna(x.median()))\n",
    "df['Amount'] = grouped['Amount'].transform(lambda x: x.fillna(x.median()))\n",
    "\n",
    "\n",
    "#After this step we still had nulls in balance and amount as there were no other records for that Account No.\n",
    "#Filling remaining NaNs after group-based imputation \n",
    "df['Balance'].fillna(df['Balance'].median(), inplace=True)\n",
    "df['Amount'].fillna(df['Amount'].median(), inplace=True)\n",
    "\n",
    "#Standardization\n",
    "scaler = StandardScaler()\n",
    "df[['Balance_scaled', 'Amount_scaled']] = scaler.fit_transform(df[['Balance', 'Amount']])\n",
    "\n",
    "#Categorical Encoding using One-Hot Encoding\n",
    "encoder = OneHotEncoder()\n",
    "encoded_cats = encoder.fit_transform(df[['Category']]).toarray()\n",
    "cat_feature_names = encoder.get_feature_names_out(['Category'])\n",
    "\n",
    "df_encoded = pd.DataFrame(encoded_cats, columns=cat_feature_names, index=df.index)\n",
    "df = pd.concat([df, df_encoded], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the optimum number of clusters needed with the Elbow Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Calculating the sum of squared distances for different numbers of clusters\n",
    "sse = []\n",
    "for k in range(1, 11):  # Adjust the range as needed\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(df[['Balance_scaled', 'Amount_scaled'] + list(cat_feature_names)])\n",
    "    sse.append(kmeans.inertia_)\n",
    "\n",
    "#Plotting SSE to find the elbow\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, 11), sse, marker='o')\n",
    "plt.title('Elbow Method For Optimal k')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Sum of squared distances')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clustering seems to be accurate with clusters ranging from 3-5. Therefore moving with 4 clusters.\n",
    "kmeans = KMeans(n_clusters=4, random_state=42)\n",
    "df['Cluster'] = kmeans.fit_predict(df[['Balance_scaled', 'Amount_scaled'] + list(cat_feature_names)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis (PCA) for Visualization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using PCA to reduce the dimensionality of your data to 2 or 3 principal components can help visualize the clusters in a two-dimensional or three-dimensional space. This approach will allow us to plot the clusters and see how they are distributed, giving a visual intuition of how distinct they are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Reducing dimensions with PCA\n",
    "pca = PCA(n_components=2)  # for 2D visualization\n",
    "principal_components = pca.fit_transform(df[['Balance_scaled', 'Amount_scaled'] + list(cat_feature_names)])\n",
    "\n",
    "#Creating a DataFrame with PCA results\n",
    "pca_df = pd.DataFrame(data=principal_components, columns=['principal component 1', 'principal component 2'])\n",
    "pca_df['Cluster'] = df['Cluster']\n",
    "\n",
    "#Plotting the clusters\n",
    "plt.figure(figsize=(8, 6))\n",
    "colors = ['r', 'g', 'b', 'c', 'm', 'y', 'k', 'orange', 'purple', 'brown']  # adjust based on number of clusters\n",
    "for i in range(kmeans.n_clusters):\n",
    "    cluster_data = pca_df[pca_df['Cluster'] == i]\n",
    "    plt.scatter(cluster_data['principal component 1'], cluster_data['principal component 2'], \n",
    "                color=colors[i], label=f'Cluster {i}', alpha=0.5)\n",
    "plt.title('Cluster visualization using PCA')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Applying DBSCAN to the standardized features\n",
    "dbscan = DBSCAN(eps=0.5, min_samples=5)  # These are common starting values for eps and min_samples\n",
    "df['DBSCAN_Cluster'] = dbscan.fit_predict(df[['Balance', 'Amount'] + list(cat_feature_names)])\n",
    "\n",
    "#Unique clusters we found \n",
    "print(f\"Unique clusters found by DBSCAN: {len(set(df['DBSCAN_Cluster']))}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time Series Forecasting (ARIMA, LSTM): Time series forecasting techniques such as ARIMA (AutoRegressive Integrated Moving Average) or LSTM (Long Short-Term Memory) neural networks can be used to predict future transaction volumes or identify seasonal trends in customer behavior. They can guide resource allocation, marketing campaign planning, and inventory management."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Customer Churn Prediction: Logistic Regression, Random Forest, or Gradient Boosting Machines (GBM).\n",
    "These models can forecast the likelihood of customers discontinuing their relationship with the bank based on their transaction history and behavior. By identifying customers at risk of churn, the bank can implement retention strategies to retain valuable customers and reduce churn rates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Learning for Anomaly Detection: Deep learning models, such as autoencoders or variational autoencoders (VAEs), could be trained on transactional data to detect anomalies or unusual patterns that may indicate fraudulent activities or outliers in customer behavior."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
